{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd06439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.10.0\n",
      "Uninstalling tensorflow-2.10.0:\n",
      "  Successfully uninstalled tensorflow-2.10.0\n",
      "Found existing installation: keras 2.10.0\n",
      "Uninstalling keras-2.10.0:\n",
      "  Successfully uninstalled keras-2.10.0\n",
      "Found existing installation: keras-rl2 1.0.5\n",
      "Uninstalling keras-rl2-1.0.5:\n",
      "  Successfully uninstalled keras-rl2-1.0.5\n",
      "Found existing installation: gymnasium 1.1.1\n",
      "Uninstalling gymnasium-1.1.1:\n",
      "  Successfully uninstalled gymnasium-1.1.1\n",
      "Found existing installation: numpy 1.24.4\n",
      "Uninstalling numpy-1.24.4:\n",
      "  Successfully uninstalled numpy-1.24.4\n",
      "Found existing installation: pygame 2.6.1\n",
      "Uninstalling pygame-2.6.1:\n",
      "  Successfully uninstalled pygame-2.6.1\n",
      "Collecting tensorflow==2.10\n",
      "  Using cached tensorflow-2.10.0-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting keras-rl2\n",
      "  Using cached keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
      "Collecting gymnasium\n",
      "  Using cached gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.6.1-cp38-cp38-win_amd64.whl.metadata (13 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (25.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (75.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (1.17.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorflow==2.10) (2.10.0)\n",
      "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10)\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from gymnasium) (8.5.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.45.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.20.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.32.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\abdullah\\desktop\\self projects\\test\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.3.1)\n",
      "Using cached tensorflow-2.10.0-cp38-cp38-win_amd64.whl (455.9 MB)\n",
      "Using cached keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
      "Using cached gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "Using cached pygame-2.6.1-cp38-cp38-win_amd64.whl (10.6 MB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras, pygame, numpy, gymnasium, tensorflow, keras-rl2\n",
      "Successfully installed gymnasium-1.1.1 keras-2.10.0 keras-rl2-1.0.5 numpy-1.24.4 pygame-2.6.1 tensorflow-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow keras keras-rl2 gymnasium numpy pygame -y\n",
    "\n",
    "!pip install tensorflow==2.10 keras-rl2 gymnasium pygame numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea9e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to adjust gymnasium env to keras-rl expected API\n",
    "class EnvWrapper(gym.Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return obs  # only observation\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        return self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e5ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create env\n",
    "env = EnvWrapper(gym.make('CartPole-v1', render_mode='human'))\n",
    "\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec403b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states)))  # Window length = 1\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))  # Q-values\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f63624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build agent\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ca42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah\\Desktop\\Self Projects\\test\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     9/50000: episode: 1, duration: 0.646s, episode steps:   9, steps per second:  14, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah\\Desktop\\Self Projects\\test\\.venv\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\Users\\Abdullah\\Desktop\\Self Projects\\test\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19/50000: episode: 2, duration: 0.524s, episode steps:  10, steps per second:  19, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.722762, mae: 0.827255, mean_q: 0.286972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah\\Desktop\\Self Projects\\test\\.venv\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    36/50000: episode: 3, duration: 0.367s, episode steps:  17, steps per second:  46, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.572289, mae: 0.731530, mean_q: 0.466274\n",
      "    45/50000: episode: 4, duration: 0.206s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.607518, mae: 0.712155, mean_q: 0.579460\n",
      "    71/50000: episode: 5, duration: 0.555s, episode steps:  26, steps per second:  47, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.466148, mae: 0.617858, mean_q: 0.689452\n",
      "    81/50000: episode: 6, duration: 0.224s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.356811, mae: 0.551854, mean_q: 0.906310\n",
      "    94/50000: episode: 7, duration: 0.286s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.312106, mae: 0.568474, mean_q: 1.123901\n",
      "   106/50000: episode: 8, duration: 0.265s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.236084, mae: 0.594175, mean_q: 1.295443\n",
      "   118/50000: episode: 9, duration: 0.267s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.208891, mae: 0.624337, mean_q: 1.438697\n",
      "   145/50000: episode: 10, duration: 0.579s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.164827, mae: 0.719290, mean_q: 1.559971\n",
      "   156/50000: episode: 11, duration: 0.249s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.112469, mae: 0.799317, mean_q: 1.756505\n",
      "   166/50000: episode: 12, duration: 0.228s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.126327, mae: 0.882722, mean_q: 1.873402\n",
      "   198/50000: episode: 13, duration: 0.675s, episode steps:  32, steps per second:  47, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.090712, mae: 0.944262, mean_q: 1.970239\n",
      "   212/50000: episode: 14, duration: 0.306s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.100038, mae: 1.051475, mean_q: 2.134718\n",
      "   228/50000: episode: 15, duration: 0.342s, episode steps:  16, steps per second:  47, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.074249, mae: 1.082400, mean_q: 2.264204\n",
      "   266/50000: episode: 16, duration: 0.802s, episode steps:  38, steps per second:  47, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.087715, mae: 1.217449, mean_q: 2.406101\n",
      "   284/50000: episode: 17, duration: 0.388s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.085670, mae: 1.317086, mean_q: 2.595779\n",
      "   299/50000: episode: 18, duration: 0.323s, episode steps:  15, steps per second:  46, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.105731, mae: 1.406430, mean_q: 2.713114\n",
      "   314/50000: episode: 19, duration: 0.330s, episode steps:  15, steps per second:  45, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.086830, mae: 1.434250, mean_q: 2.842499\n",
      "   332/50000: episode: 20, duration: 0.854s, episode steps:  18, steps per second:  21, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.115846, mae: 1.510843, mean_q: 2.920127\n",
      "   358/50000: episode: 21, duration: 0.555s, episode steps:  26, steps per second:  47, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.097715, mae: 1.588331, mean_q: 3.108868\n",
      "   387/50000: episode: 22, duration: 0.614s, episode steps:  29, steps per second:  47, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.690 [0.000, 1.000],  loss: 0.157238, mae: 1.703866, mean_q: 3.282079\n",
      "   419/50000: episode: 23, duration: 0.678s, episode steps:  32, steps per second:  47, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.176710, mae: 1.843256, mean_q: 3.523392\n",
      "   429/50000: episode: 24, duration: 0.221s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.136860, mae: 1.900898, mean_q: 3.633411\n",
      "   463/50000: episode: 25, duration: 0.718s, episode steps:  34, steps per second:  47, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.202351, mae: 1.993023, mean_q: 3.808483\n",
      "   475/50000: episode: 26, duration: 0.265s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.231385, mae: 2.108880, mean_q: 4.000305\n",
      "   490/50000: episode: 27, duration: 0.327s, episode steps:  15, steps per second:  46, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.206028, mae: 2.147651, mean_q: 4.070288\n",
      "   504/50000: episode: 28, duration: 0.305s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.200333, mae: 2.195899, mean_q: 4.227340\n",
      "   529/50000: episode: 29, duration: 0.530s, episode steps:  25, steps per second:  47, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.162409, mae: 2.253491, mean_q: 4.342529\n",
      "   547/50000: episode: 30, duration: 0.390s, episode steps:  18, steps per second:  46, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.202103, mae: 2.368121, mean_q: 4.553378\n",
      "   557/50000: episode: 31, duration: 0.898s, episode steps:  10, steps per second:  11, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.233419, mae: 2.393786, mean_q: 4.512359\n",
      "   575/50000: episode: 32, duration: 0.384s, episode steps:  18, steps per second:  47, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.283029, mae: 2.467549, mean_q: 4.701366\n",
      "   588/50000: episode: 33, duration: 0.288s, episode steps:  13, steps per second:  45, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.231132, mae: 2.507224, mean_q: 4.754624\n",
      "   608/50000: episode: 34, duration: 0.433s, episode steps:  20, steps per second:  46, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.236200, mae: 2.598204, mean_q: 4.923640\n",
      "   625/50000: episode: 35, duration: 0.368s, episode steps:  17, steps per second:  46, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.257428, mae: 2.651378, mean_q: 5.003712\n",
      "   640/50000: episode: 36, duration: 0.330s, episode steps:  15, steps per second:  46, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.229606, mae: 2.704256, mean_q: 5.122461\n",
      "   651/50000: episode: 37, duration: 0.246s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.336865, mae: 2.809924, mean_q: 5.326766\n",
      "   666/50000: episode: 38, duration: 0.327s, episode steps:  15, steps per second:  46, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.192713, mae: 2.803096, mean_q: 5.363868\n",
      "   678/50000: episode: 39, duration: 0.267s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.309831, mae: 2.856995, mean_q: 5.440882\n",
      "   710/50000: episode: 40, duration: 0.673s, episode steps:  32, steps per second:  48, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.371082, mae: 2.935714, mean_q: 5.531548\n",
      "   721/50000: episode: 41, duration: 0.245s, episode steps:  11, steps per second:  45, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.479638, mae: 3.005927, mean_q: 5.586642\n",
      "   741/50000: episode: 42, duration: 0.431s, episode steps:  20, steps per second:  46, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.427729, mae: 3.083479, mean_q: 5.789463\n",
      "   758/50000: episode: 43, duration: 0.369s, episode steps:  17, steps per second:  46, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.307284, mae: 3.121722, mean_q: 5.964989\n",
      "   781/50000: episode: 44, duration: 0.494s, episode steps:  23, steps per second:  47, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.508050, mae: 3.222025, mean_q: 6.034064\n",
      "   797/50000: episode: 45, duration: 0.348s, episode steps:  16, steps per second:  46, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.538188, mae: 3.245433, mean_q: 5.997815\n",
      "   816/50000: episode: 46, duration: 0.410s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.408244, mae: 3.358384, mean_q: 6.384136\n",
      "   835/50000: episode: 47, duration: 0.416s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.389589, mae: 3.419879, mean_q: 6.529982\n",
      "   845/50000: episode: 48, duration: 0.225s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.613608, mae: 3.442412, mean_q: 6.405698\n",
      "   918/50000: episode: 49, duration: 1.510s, episode steps:  73, steps per second:  48, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.420652, mae: 3.569845, mean_q: 6.748189\n",
      "   956/50000: episode: 50, duration: 0.802s, episode steps:  38, steps per second:  47, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.428517, mae: 3.756385, mean_q: 7.204855\n",
      "  1005/50000: episode: 51, duration: 1.032s, episode steps:  49, steps per second:  47, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.473947, mae: 3.942227, mean_q: 7.568778\n",
      "  1024/50000: episode: 52, duration: 0.409s, episode steps:  19, steps per second:  46, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.528005, mae: 4.061633, mean_q: 7.737596\n",
      "  1033/50000: episode: 53, duration: 0.203s, episode steps:   9, steps per second:  44, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.534187, mae: 4.137686, mean_q: 7.881167\n",
      "  1046/50000: episode: 54, duration: 0.287s, episode steps:  13, steps per second:  45, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.489668, mae: 4.146655, mean_q: 7.963814\n",
      "  1074/50000: episode: 55, duration: 0.594s, episode steps:  28, steps per second:  47, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.570286, mae: 4.252337, mean_q: 8.210565\n",
      "  1101/50000: episode: 56, duration: 0.573s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.524888, mae: 4.330654, mean_q: 8.358162\n",
      "  1113/50000: episode: 57, duration: 0.266s, episode steps:  12, steps per second:  45, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.491864, mae: 4.402230, mean_q: 8.509711\n",
      "  1126/50000: episode: 58, duration: 0.285s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.526455, mae: 4.499933, mean_q: 8.692307\n",
      "  1151/50000: episode: 59, duration: 0.531s, episode steps:  25, steps per second:  47, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.576217, mae: 4.522838, mean_q: 8.697570\n",
      "  1179/50000: episode: 60, duration: 0.594s, episode steps:  28, steps per second:  47, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.732642, mae: 4.620859, mean_q: 8.868704\n",
      "  1220/50000: episode: 61, duration: 0.867s, episode steps:  41, steps per second:  47, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 0.643268, mae: 4.683820, mean_q: 9.055828\n",
      "  1242/50000: episode: 62, duration: 0.474s, episode steps:  22, steps per second:  46, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.980442, mae: 4.854278, mean_q: 9.223942\n",
      "  1256/50000: episode: 63, duration: 0.307s, episode steps:  14, steps per second:  46, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.515538, mae: 4.964717, mean_q: 9.624466\n",
      "  1302/50000: episode: 64, duration: 0.967s, episode steps:  46, steps per second:  48, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.703890, mae: 5.010623, mean_q: 9.655217\n",
      "  1329/50000: episode: 65, duration: 0.579s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.852576, mae: 5.095348, mean_q: 9.818124\n",
      "  1363/50000: episode: 66, duration: 0.716s, episode steps:  34, steps per second:  47, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.603463, mae: 5.233956, mean_q: 10.200455\n",
      "  1406/50000: episode: 67, duration: 0.898s, episode steps:  43, steps per second:  48, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.923508, mae: 5.320291, mean_q: 10.316317\n",
      "  1418/50000: episode: 68, duration: 0.263s, episode steps:  12, steps per second:  46, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.929365, mae: 5.389528, mean_q: 10.347629\n",
      "  1445/50000: episode: 69, duration: 0.572s, episode steps:  27, steps per second:  47, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 0.638036, mae: 5.508309, mean_q: 10.786201\n",
      "  1493/50000: episode: 70, duration: 1.000s, episode steps:  48, steps per second:  48, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.670835, mae: 5.614812, mean_q: 10.949212\n",
      "  1546/50000: episode: 71, duration: 1.110s, episode steps:  53, steps per second:  48, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.858860, mae: 5.830105, mean_q: 11.338539\n",
      "  1568/50000: episode: 72, duration: 0.468s, episode steps:  22, steps per second:  47, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.855124, mae: 6.012294, mean_q: 11.792497\n",
      "  1591/50000: episode: 73, duration: 0.486s, episode steps:  23, steps per second:  47, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.738900, mae: 6.053610, mean_q: 11.901517\n",
      "  1622/50000: episode: 74, duration: 0.655s, episode steps:  31, steps per second:  47, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.946902, mae: 6.110177, mean_q: 11.934543\n",
      "  1647/50000: episode: 75, duration: 0.530s, episode steps:  25, steps per second:  47, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.052752, mae: 6.222717, mean_q: 12.149286\n",
      "  1741/50000: episode: 76, duration: 1.954s, episode steps:  94, steps per second:  48, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.842334, mae: 6.434714, mean_q: 12.678804\n",
      "  1812/50000: episode: 77, duration: 1.478s, episode steps:  71, steps per second:  48, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.871322, mae: 6.734202, mean_q: 13.363317\n",
      "  1895/50000: episode: 78, duration: 1.723s, episode steps:  83, steps per second:  48, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.857216, mae: 7.026133, mean_q: 14.026223\n",
      "  1967/50000: episode: 79, duration: 1.802s, episode steps:  72, steps per second:  40, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.751422, mae: 7.352539, mean_q: 14.733519\n",
      "  2035/50000: episode: 80, duration: 1.419s, episode steps:  68, steps per second:  48, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.106272, mae: 7.621746, mean_q: 15.134886\n",
      "  2111/50000: episode: 81, duration: 1.841s, episode steps:  76, steps per second:  41, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.847343, mae: 7.914714, mean_q: 15.861270\n",
      "  2240/50000: episode: 82, duration: 2.672s, episode steps: 129, steps per second:  48, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.059627, mae: 8.315224, mean_q: 16.673693\n",
      "  2308/50000: episode: 83, duration: 1.412s, episode steps:  68, steps per second:  48, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 0.778842, mae: 8.728195, mean_q: 17.529266\n",
      "  2398/50000: episode: 84, duration: 1.863s, episode steps:  90, steps per second:  48, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.147668, mae: 8.985454, mean_q: 18.068016\n",
      "  2561/50000: episode: 85, duration: 3.368s, episode steps: 163, steps per second:  48, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.957927, mae: 9.436808, mean_q: 19.122339\n",
      "  2761/50000: episode: 86, duration: 4.134s, episode steps: 200, steps per second:  48, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.182998, mae: 10.280610, mean_q: 20.880348\n",
      "  2902/50000: episode: 87, duration: 2.910s, episode steps: 141, steps per second:  48, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.508869, mae: 11.027123, mean_q: 22.402157\n",
      "  3053/50000: episode: 88, duration: 3.120s, episode steps: 151, steps per second:  48, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.530677, mae: 11.587244, mean_q: 23.506142\n",
      "  3199/50000: episode: 89, duration: 3.011s, episode steps: 146, steps per second:  48, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.319174, mae: 12.318423, mean_q: 25.091972\n",
      "  3366/50000: episode: 90, duration: 3.446s, episode steps: 167, steps per second:  48, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 1.450932, mae: 13.001493, mean_q: 26.501268\n",
      "  3606/50000: episode: 91, duration: 4.956s, episode steps: 240, steps per second:  48, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.546313, mae: 13.935090, mean_q: 28.426216\n",
      "  3727/50000: episode: 92, duration: 2.511s, episode steps: 121, steps per second:  48, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.702794, mae: 14.678853, mean_q: 29.945976\n",
      "  3882/50000: episode: 93, duration: 3.203s, episode steps: 155, steps per second:  48, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.673952, mae: 15.162590, mean_q: 30.926191\n",
      "  4102/50000: episode: 94, duration: 4.546s, episode steps: 220, steps per second:  48, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.883291, mae: 16.097824, mean_q: 32.846188\n",
      "  4230/50000: episode: 95, duration: 2.646s, episode steps: 128, steps per second:  48, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 1.551461, mae: 16.761997, mean_q: 34.171303\n",
      "  4375/50000: episode: 96, duration: 2.996s, episode steps: 145, steps per second:  48, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.002462, mae: 17.389046, mean_q: 35.488213\n",
      "  4514/50000: episode: 97, duration: 2.873s, episode steps: 139, steps per second:  48, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.899836, mae: 18.006762, mean_q: 36.725327\n",
      "  4683/50000: episode: 98, duration: 3.484s, episode steps: 169, steps per second:  49, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.421158, mae: 18.450874, mean_q: 37.539749\n",
      "  4851/50000: episode: 99, duration: 3.475s, episode steps: 168, steps per second:  48, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.423466, mae: 19.336571, mean_q: 39.370750\n",
      "  5080/50000: episode: 100, duration: 4.699s, episode steps: 229, steps per second:  49, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.297741, mae: 20.019436, mean_q: 40.788261\n",
      "  5242/50000: episode: 101, duration: 3.348s, episode steps: 162, steps per second:  48, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.734788, mae: 20.666130, mean_q: 42.025108\n",
      "  5433/50000: episode: 102, duration: 3.951s, episode steps: 191, steps per second:  48, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.246694, mae: 21.206444, mean_q: 43.207123\n",
      "  5679/50000: episode: 103, duration: 5.071s, episode steps: 246, steps per second:  49, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.754814, mae: 21.979395, mean_q: 44.695820\n",
      "  5841/50000: episode: 104, duration: 3.344s, episode steps: 162, steps per second:  48, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2.757039, mae: 22.918941, mean_q: 46.650368\n",
      "  5993/50000: episode: 105, duration: 3.138s, episode steps: 152, steps per second:  48, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 1.992644, mae: 23.111923, mean_q: 47.143753\n",
      "  6188/50000: episode: 106, duration: 4.023s, episode steps: 195, steps per second:  48, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.547050, mae: 23.918306, mean_q: 48.820637\n",
      "  6409/50000: episode: 107, duration: 4.565s, episode steps: 221, steps per second:  48, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.010832, mae: 24.662100, mean_q: 50.350788\n",
      "  6781/50000: episode: 108, duration: 7.640s, episode steps: 372, steps per second:  49, episode reward: 372.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.744493, mae: 25.911133, mean_q: 52.714382\n",
      "  6969/50000: episode: 109, duration: 3.881s, episode steps: 188, steps per second:  48, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 3.915581, mae: 26.769833, mean_q: 54.362118\n",
      "  7137/50000: episode: 110, duration: 3.475s, episode steps: 168, steps per second:  48, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.484607, mae: 27.260153, mean_q: 55.490376\n",
      "  7302/50000: episode: 111, duration: 3.404s, episode steps: 165, steps per second:  48, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.345755, mae: 27.752266, mean_q: 56.498863\n",
      "  7496/50000: episode: 112, duration: 3.996s, episode steps: 194, steps per second:  49, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.480556, mae: 28.158827, mean_q: 57.207146\n",
      "  7655/50000: episode: 113, duration: 3.282s, episode steps: 159, steps per second:  48, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.196111, mae: 28.674486, mean_q: 58.309280\n",
      "  7806/50000: episode: 114, duration: 3.111s, episode steps: 151, steps per second:  49, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.816569, mae: 29.319820, mean_q: 59.561089\n",
      "  8047/50000: episode: 115, duration: 4.967s, episode steps: 241, steps per second:  49, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.141798, mae: 29.885017, mean_q: 60.586098\n",
      "  8290/50000: episode: 116, duration: 5.004s, episode steps: 243, steps per second:  49, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3.348936, mae: 30.286526, mean_q: 61.515396\n",
      "  8670/50000: episode: 117, duration: 7.824s, episode steps: 380, steps per second:  49, episode reward: 380.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 4.344991, mae: 31.348391, mean_q: 63.516449\n",
      "  8848/50000: episode: 118, duration: 3.656s, episode steps: 178, steps per second:  49, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.472861, mae: 32.434097, mean_q: 65.563522\n",
      "  9076/50000: episode: 119, duration: 4.708s, episode steps: 228, steps per second:  48, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.491655, mae: 32.521866, mean_q: 65.903206\n",
      "  9271/50000: episode: 120, duration: 4.024s, episode steps: 195, steps per second:  48, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.885730, mae: 33.164566, mean_q: 67.303589\n",
      "  9481/50000: episode: 121, duration: 4.320s, episode steps: 210, steps per second:  49, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.126419, mae: 33.668259, mean_q: 68.222511\n",
      "  9703/50000: episode: 122, duration: 4.580s, episode steps: 222, steps per second:  48, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.123608, mae: 34.085991, mean_q: 69.083107\n",
      "  9878/50000: episode: 123, duration: 3.609s, episode steps: 175, steps per second:  48, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.122034, mae: 34.642136, mean_q: 70.126984\n",
      " 10092/50000: episode: 124, duration: 4.407s, episode steps: 214, steps per second:  49, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.234138, mae: 34.842457, mean_q: 70.504623\n",
      " 10265/50000: episode: 125, duration: 3.573s, episode steps: 173, steps per second:  48, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.390121, mae: 35.057865, mean_q: 70.874107\n",
      " 10439/50000: episode: 126, duration: 3.598s, episode steps: 174, steps per second:  48, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 4.164812, mae: 35.388859, mean_q: 71.839874\n",
      " 10599/50000: episode: 127, duration: 3.296s, episode steps: 160, steps per second:  49, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 5.572422, mae: 35.726002, mean_q: 72.381027\n",
      " 10823/50000: episode: 128, duration: 4.619s, episode steps: 224, steps per second:  48, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.995071, mae: 36.497486, mean_q: 73.820389\n",
      " 10980/50000: episode: 129, duration: 3.232s, episode steps: 157, steps per second:  49, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 6.201443, mae: 36.279228, mean_q: 73.316681\n",
      " 11184/50000: episode: 130, duration: 4.195s, episode steps: 204, steps per second:  49, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4.201574, mae: 36.890488, mean_q: 74.454826\n",
      " 11349/50000: episode: 131, duration: 3.413s, episode steps: 165, steps per second:  48, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 5.845385, mae: 37.228271, mean_q: 75.273293\n",
      " 11521/50000: episode: 132, duration: 3.551s, episode steps: 172, steps per second:  48, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.660675, mae: 37.263313, mean_q: 75.109749\n",
      " 11690/50000: episode: 133, duration: 3.485s, episode steps: 169, steps per second:  48, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.939936, mae: 37.092686, mean_q: 74.851494\n",
      " 11870/50000: episode: 134, duration: 3.710s, episode steps: 180, steps per second:  49, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.769279, mae: 37.736217, mean_q: 76.311211\n",
      " 12029/50000: episode: 135, duration: 3.289s, episode steps: 159, steps per second:  48, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 4.522287, mae: 37.754089, mean_q: 76.321747\n",
      " 12200/50000: episode: 136, duration: 3.533s, episode steps: 171, steps per second:  48, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.336739, mae: 38.174404, mean_q: 77.105476\n",
      " 12363/50000: episode: 137, duration: 3.373s, episode steps: 163, steps per second:  48, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 5.051107, mae: 38.514465, mean_q: 77.677490\n",
      " 12588/50000: episode: 138, duration: 4.634s, episode steps: 225, steps per second:  49, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.702400, mae: 38.305286, mean_q: 77.307228\n",
      " 12779/50000: episode: 139, duration: 3.949s, episode steps: 191, steps per second:  48, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.595467, mae: 38.848503, mean_q: 78.441963\n",
      " 12986/50000: episode: 140, duration: 4.262s, episode steps: 207, steps per second:  49, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.345853, mae: 38.875938, mean_q: 78.551117\n",
      " 13205/50000: episode: 141, duration: 4.507s, episode steps: 219, steps per second:  49, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.914012, mae: 39.602066, mean_q: 79.920731\n",
      " 13391/50000: episode: 142, duration: 3.827s, episode steps: 186, steps per second:  49, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.713671, mae: 39.337963, mean_q: 79.462357\n",
      " 13713/50000: episode: 143, duration: 6.622s, episode steps: 322, steps per second:  49, episode reward: 322.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 7.319611, mae: 39.845600, mean_q: 80.346336\n",
      " 13856/50000: episode: 144, duration: 2.955s, episode steps: 143, steps per second:  48, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.208424, mae: 40.233761, mean_q: 81.033615\n",
      " 14029/50000: episode: 145, duration: 3.574s, episode steps: 173, steps per second:  48, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.631966, mae: 40.535976, mean_q: 81.805275\n",
      " 14196/50000: episode: 146, duration: 3.441s, episode steps: 167, steps per second:  49, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.543419, mae: 40.720245, mean_q: 82.178627\n",
      " 14397/50000: episode: 147, duration: 4.141s, episode steps: 201, steps per second:  49, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 5.472367, mae: 40.270954, mean_q: 81.199280\n",
      " 14554/50000: episode: 148, duration: 3.243s, episode steps: 157, steps per second:  48, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 6.400406, mae: 40.624390, mean_q: 81.808372\n",
      " 14713/50000: episode: 149, duration: 3.281s, episode steps: 159, steps per second:  48, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 7.966636, mae: 40.106163, mean_q: 80.774223\n",
      " 14876/50000: episode: 150, duration: 3.368s, episode steps: 163, steps per second:  48, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 6.562031, mae: 40.511127, mean_q: 81.553780\n",
      " 15055/50000: episode: 151, duration: 3.693s, episode steps: 179, steps per second:  48, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.191875, mae: 40.987698, mean_q: 82.751114\n",
      " 15244/50000: episode: 152, duration: 3.881s, episode steps: 189, steps per second:  49, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.077902, mae: 40.723503, mean_q: 82.172569\n",
      " 15453/50000: episode: 153, duration: 4.303s, episode steps: 209, steps per second:  49, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.692658, mae: 41.268028, mean_q: 83.368866\n",
      " 15646/50000: episode: 154, duration: 3.980s, episode steps: 193, steps per second:  48, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 3.903835, mae: 40.965813, mean_q: 82.715599\n",
      " 15855/50000: episode: 155, duration: 4.299s, episode steps: 209, steps per second:  49, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.806121, mae: 40.862423, mean_q: 82.446564\n",
      " 16240/50000: episode: 156, duration: 7.930s, episode steps: 385, steps per second:  49, episode reward: 385.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.071481, mae: 41.670773, mean_q: 84.010803\n",
      " 16425/50000: episode: 157, duration: 3.820s, episode steps: 185, steps per second:  48, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.664761, mae: 41.768295, mean_q: 84.194756\n",
      " 16638/50000: episode: 158, duration: 4.393s, episode steps: 213, steps per second:  48, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5.029849, mae: 41.929058, mean_q: 84.564987\n",
      " 16838/50000: episode: 159, duration: 4.138s, episode steps: 200, steps per second:  48, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.813844, mae: 41.752045, mean_q: 84.112106\n",
      " 17015/50000: episode: 160, duration: 3.655s, episode steps: 177, steps per second:  48, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.262184, mae: 41.733608, mean_q: 84.214066\n",
      " 17190/50000: episode: 161, duration: 3.604s, episode steps: 175, steps per second:  49, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.438443, mae: 42.247875, mean_q: 85.247681\n",
      " 17411/50000: episode: 162, duration: 4.571s, episode steps: 221, steps per second:  48, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.800774, mae: 42.040272, mean_q: 84.729958\n",
      " 17594/50000: episode: 163, duration: 3.774s, episode steps: 183, steps per second:  48, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.819012, mae: 41.946571, mean_q: 84.584648\n",
      " 17759/50000: episode: 164, duration: 3.403s, episode steps: 165, steps per second:  48, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.791886, mae: 41.816914, mean_q: 84.434151\n",
      " 17945/50000: episode: 165, duration: 3.828s, episode steps: 186, steps per second:  49, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 4.362041, mae: 41.981071, mean_q: 84.631882\n",
      " 18161/50000: episode: 166, duration: 4.447s, episode steps: 216, steps per second:  49, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.692533, mae: 42.496975, mean_q: 85.600243\n",
      " 18390/50000: episode: 167, duration: 4.717s, episode steps: 229, steps per second:  49, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.617545, mae: 42.072094, mean_q: 84.769020\n",
      " 18574/50000: episode: 168, duration: 3.812s, episode steps: 184, steps per second:  48, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6.313290, mae: 42.625359, mean_q: 85.769096\n",
      " 18786/50000: episode: 169, duration: 4.364s, episode steps: 212, steps per second:  49, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.404522, mae: 42.618370, mean_q: 85.822784\n",
      " 19024/50000: episode: 170, duration: 4.915s, episode steps: 238, steps per second:  48, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.533781, mae: 42.346325, mean_q: 85.361160\n",
      " 19182/50000: episode: 171, duration: 3.262s, episode steps: 158, steps per second:  48, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.475512, mae: 42.441040, mean_q: 85.529755\n",
      " 19371/50000: episode: 172, duration: 3.900s, episode steps: 189, steps per second:  48, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 3.259116, mae: 42.973682, mean_q: 86.566170\n",
      " 19574/50000: episode: 173, duration: 4.187s, episode steps: 203, steps per second:  48, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 5.525644, mae: 42.693592, mean_q: 86.098793\n",
      " 19915/50000: episode: 174, duration: 7.010s, episode steps: 341, steps per second:  49, episode reward: 341.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.955191, mae: 43.116306, mean_q: 86.774818\n",
      " 20115/50000: episode: 175, duration: 4.120s, episode steps: 200, steps per second:  49, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.585795, mae: 43.371288, mean_q: 87.392426\n",
      " 20328/50000: episode: 176, duration: 4.390s, episode steps: 213, steps per second:  49, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 4.290884, mae: 43.233009, mean_q: 87.229530\n",
      " 20615/50000: episode: 177, duration: 5.910s, episode steps: 287, steps per second:  49, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 5.411438, mae: 43.502361, mean_q: 87.496460\n",
      " 20871/50000: episode: 178, duration: 5.281s, episode steps: 256, steps per second:  48, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.324442, mae: 43.661755, mean_q: 87.740829\n",
      " 21145/50000: episode: 179, duration: 5.639s, episode steps: 274, steps per second:  49, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 6.314446, mae: 43.819038, mean_q: 88.124725\n",
      " 21378/50000: episode: 180, duration: 4.811s, episode steps: 233, steps per second:  48, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4.705885, mae: 44.666428, mean_q: 89.722183\n",
      " 21555/50000: episode: 181, duration: 3.652s, episode steps: 177, steps per second:  48, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.956830, mae: 44.027805, mean_q: 88.328552\n",
      " 21732/50000: episode: 182, duration: 3.649s, episode steps: 177, steps per second:  49, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.206831, mae: 44.643719, mean_q: 89.780403\n",
      " 21933/50000: episode: 183, duration: 4.149s, episode steps: 201, steps per second:  48, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 6.391318, mae: 44.674538, mean_q: 89.737701\n",
      " 22101/50000: episode: 184, duration: 3.462s, episode steps: 168, steps per second:  49, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 2.661507, mae: 44.228558, mean_q: 89.177162\n",
      " 22385/50000: episode: 185, duration: 5.848s, episode steps: 284, steps per second:  49, episode reward: 284.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.739817, mae: 44.307430, mean_q: 89.236298\n",
      " 22576/50000: episode: 186, duration: 3.934s, episode steps: 191, steps per second:  49, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.757699, mae: 44.549103, mean_q: 89.633072\n",
      " 22748/50000: episode: 187, duration: 3.536s, episode steps: 172, steps per second:  49, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 4.377645, mae: 44.186253, mean_q: 88.848167\n",
      " 22916/50000: episode: 188, duration: 3.464s, episode steps: 168, steps per second:  49, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.504114, mae: 44.200371, mean_q: 89.009895\n",
      " 23116/50000: episode: 189, duration: 4.137s, episode steps: 200, steps per second:  48, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.320680, mae: 44.816284, mean_q: 90.140076\n",
      " 23313/50000: episode: 190, duration: 4.072s, episode steps: 197, steps per second:  48, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.734938, mae: 44.290024, mean_q: 89.020050\n",
      " 23480/50000: episode: 191, duration: 3.443s, episode steps: 167, steps per second:  48, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 3.359173, mae: 44.853420, mean_q: 90.322906\n",
      " 23728/50000: episode: 192, duration: 5.125s, episode steps: 248, steps per second:  48, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3.771147, mae: 44.788338, mean_q: 89.954857\n",
      " 24088/50000: episode: 193, duration: 7.417s, episode steps: 360, steps per second:  49, episode reward: 360.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.837112, mae: 44.821198, mean_q: 90.193207\n",
      " 24309/50000: episode: 194, duration: 4.558s, episode steps: 221, steps per second:  48, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.544616, mae: 44.829861, mean_q: 90.108276\n",
      " 24520/50000: episode: 195, duration: 4.353s, episode steps: 211, steps per second:  48, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.696039, mae: 44.778816, mean_q: 90.238441\n",
      " 24694/50000: episode: 196, duration: 3.590s, episode steps: 174, steps per second:  48, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.454 [0.000, 1.000],  loss: 2.639917, mae: 44.618252, mean_q: 89.953178\n",
      " 24903/50000: episode: 197, duration: 4.305s, episode steps: 209, steps per second:  49, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.959560, mae: 44.979683, mean_q: 90.449379\n",
      " 25140/50000: episode: 198, duration: 4.874s, episode steps: 237, steps per second:  49, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.463738, mae: 44.893719, mean_q: 90.344841\n",
      " 25321/50000: episode: 199, duration: 3.734s, episode steps: 181, steps per second:  48, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.692560, mae: 44.652653, mean_q: 89.738739\n",
      " 25529/50000: episode: 200, duration: 4.297s, episode steps: 208, steps per second:  48, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 4.222967, mae: 45.198025, mean_q: 90.961594\n",
      " 25678/50000: episode: 201, duration: 3.082s, episode steps: 149, steps per second:  48, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 3.039839, mae: 45.188786, mean_q: 90.954552\n",
      " 25821/50000: episode: 202, duration: 2.956s, episode steps: 143, steps per second:  48, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.566 [0.000, 1.000],  loss: 3.696030, mae: 45.191704, mean_q: 91.116394\n",
      " 26089/50000: episode: 203, duration: 5.527s, episode steps: 268, steps per second:  48, episode reward: 268.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.353786, mae: 45.393284, mean_q: 91.354622\n",
      " 26313/50000: episode: 204, duration: 4.623s, episode steps: 224, steps per second:  48, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.217395, mae: 45.690681, mean_q: 91.955254\n",
      " 26496/50000: episode: 205, duration: 3.779s, episode steps: 183, steps per second:  48, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 3.221053, mae: 46.285091, mean_q: 93.156487\n",
      " 26734/50000: episode: 206, duration: 4.901s, episode steps: 238, steps per second:  49, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.757952, mae: 45.619759, mean_q: 91.855423\n",
      " 26983/50000: episode: 207, duration: 5.133s, episode steps: 249, steps per second:  49, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.299378, mae: 46.148693, mean_q: 92.740044\n",
      " 27165/50000: episode: 208, duration: 3.745s, episode steps: 182, steps per second:  49, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.683545, mae: 45.422058, mean_q: 91.336433\n",
      " 27343/50000: episode: 209, duration: 3.671s, episode steps: 178, steps per second:  48, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.933653, mae: 45.831921, mean_q: 92.336922\n",
      " 27581/50000: episode: 210, duration: 4.892s, episode steps: 238, steps per second:  49, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.294728, mae: 45.757671, mean_q: 92.033653\n",
      " 27825/50000: episode: 211, duration: 5.039s, episode steps: 244, steps per second:  48, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.241087, mae: 45.964405, mean_q: 92.484283\n",
      " 27987/50000: episode: 212, duration: 3.346s, episode steps: 162, steps per second:  48, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.814863, mae: 46.269482, mean_q: 92.982590\n",
      " 28226/50000: episode: 213, duration: 4.912s, episode steps: 239, steps per second:  49, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.781491, mae: 46.244755, mean_q: 93.153374\n",
      " 28529/50000: episode: 214, duration: 6.245s, episode steps: 303, steps per second:  49, episode reward: 303.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.928702, mae: 46.149952, mean_q: 92.989716\n",
      " 28799/50000: episode: 215, duration: 5.566s, episode steps: 270, steps per second:  49, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.080815, mae: 46.474125, mean_q: 93.623787\n",
      " 28979/50000: episode: 216, duration: 3.707s, episode steps: 180, steps per second:  49, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.220719, mae: 45.776157, mean_q: 92.239235\n",
      " 29198/50000: episode: 217, duration: 4.514s, episode steps: 219, steps per second:  49, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.410562, mae: 46.143055, mean_q: 92.954353\n",
      " 29368/50000: episode: 218, duration: 3.498s, episode steps: 170, steps per second:  49, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 2.427262, mae: 46.251919, mean_q: 93.120956\n",
      " 29536/50000: episode: 219, duration: 3.464s, episode steps: 168, steps per second:  49, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.158581, mae: 46.314884, mean_q: 93.243057\n",
      " 29723/50000: episode: 220, duration: 3.855s, episode steps: 187, steps per second:  49, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.019471, mae: 46.725277, mean_q: 94.151787\n",
      " 29919/50000: episode: 221, duration: 4.036s, episode steps: 196, steps per second:  49, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.725969, mae: 46.349060, mean_q: 93.357803\n",
      " 30121/50000: episode: 222, duration: 4.176s, episode steps: 202, steps per second:  48, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.293256, mae: 46.108040, mean_q: 92.833702\n",
      " 30279/50000: episode: 223, duration: 3.265s, episode steps: 158, steps per second:  48, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.638504, mae: 46.685913, mean_q: 94.049011\n",
      " 30554/50000: episode: 224, duration: 5.673s, episode steps: 275, steps per second:  48, episode reward: 275.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.394016, mae: 46.359276, mean_q: 93.510338\n",
      " 30782/50000: episode: 225, duration: 4.695s, episode steps: 228, steps per second:  49, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.838733, mae: 46.048340, mean_q: 92.805267\n",
      " 30963/50000: episode: 226, duration: 3.733s, episode steps: 181, steps per second:  48, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.246206, mae: 46.434761, mean_q: 93.444305\n",
      " 31100/50000: episode: 227, duration: 2.827s, episode steps: 137, steps per second:  48, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 2.357077, mae: 46.370850, mean_q: 93.442986\n",
      " 31293/50000: episode: 228, duration: 3.974s, episode steps: 193, steps per second:  49, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 2.421809, mae: 46.280045, mean_q: 93.305527\n",
      " 31479/50000: episode: 229, duration: 3.818s, episode steps: 186, steps per second:  49, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.457990, mae: 45.791912, mean_q: 92.257751\n",
      " 31695/50000: episode: 230, duration: 4.443s, episode steps: 216, steps per second:  49, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.569010, mae: 46.586739, mean_q: 93.739243\n",
      " 31871/50000: episode: 231, duration: 3.638s, episode steps: 176, steps per second:  48, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 2.513864, mae: 46.491745, mean_q: 93.645309\n",
      " 32100/50000: episode: 232, duration: 4.718s, episode steps: 229, steps per second:  49, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.514353, mae: 46.456791, mean_q: 93.541382\n",
      " 32242/50000: episode: 233, duration: 2.932s, episode steps: 142, steps per second:  48, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 1.970409, mae: 46.629272, mean_q: 93.968315\n",
      " 32402/50000: episode: 234, duration: 3.309s, episode steps: 160, steps per second:  48, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.018809, mae: 46.563786, mean_q: 93.706505\n",
      " 32598/50000: episode: 235, duration: 4.037s, episode steps: 196, steps per second:  49, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.885079, mae: 46.178112, mean_q: 92.888489\n",
      " 32757/50000: episode: 236, duration: 3.277s, episode steps: 159, steps per second:  49, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 2.706984, mae: 45.900352, mean_q: 92.398415\n",
      " 32913/50000: episode: 237, duration: 3.218s, episode steps: 156, steps per second:  48, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 3.324673, mae: 46.037292, mean_q: 92.618240\n",
      " 33077/50000: episode: 238, duration: 3.390s, episode steps: 164, steps per second:  48, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.360898, mae: 46.245098, mean_q: 93.167496\n",
      " 33236/50000: episode: 239, duration: 3.273s, episode steps: 159, steps per second:  49, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 3.595707, mae: 46.016335, mean_q: 92.501450\n",
      " 33388/50000: episode: 240, duration: 3.135s, episode steps: 152, steps per second:  48, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 3.201085, mae: 45.383968, mean_q: 91.226204\n",
      " 33541/50000: episode: 241, duration: 3.156s, episode steps: 153, steps per second:  48, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3.532173, mae: 46.386528, mean_q: 93.189400\n",
      " 33671/50000: episode: 242, duration: 2.687s, episode steps: 130, steps per second:  48, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.287869, mae: 46.472084, mean_q: 93.542336\n",
      " 33857/50000: episode: 243, duration: 3.845s, episode steps: 186, steps per second:  48, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.386538, mae: 45.516182, mean_q: 91.584061\n",
      " 34043/50000: episode: 244, duration: 3.840s, episode steps: 186, steps per second:  48, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.060085, mae: 45.954395, mean_q: 92.696960\n",
      " 34182/50000: episode: 245, duration: 2.880s, episode steps: 139, steps per second:  48, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.998933, mae: 45.210365, mean_q: 91.165756\n",
      " 34403/50000: episode: 246, duration: 4.556s, episode steps: 221, steps per second:  49, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.916257, mae: 45.818897, mean_q: 92.267975\n",
      " 34660/50000: episode: 247, duration: 5.284s, episode steps: 257, steps per second:  49, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.530520, mae: 45.911732, mean_q: 92.419975\n",
      " 34819/50000: episode: 248, duration: 3.285s, episode steps: 159, steps per second:  48, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.165614, mae: 45.926716, mean_q: 92.396523\n",
      " 35035/50000: episode: 249, duration: 4.454s, episode steps: 216, steps per second:  48, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.219280, mae: 45.364998, mean_q: 91.290382\n",
      " 35268/50000: episode: 250, duration: 4.804s, episode steps: 233, steps per second:  48, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.152986, mae: 45.444454, mean_q: 91.612930\n",
      " 35415/50000: episode: 251, duration: 3.032s, episode steps: 147, steps per second:  48, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.738096, mae: 45.114655, mean_q: 90.995674\n",
      " 35586/50000: episode: 252, duration: 3.527s, episode steps: 171, steps per second:  48, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.228627, mae: 45.862789, mean_q: 92.336296\n",
      " 35744/50000: episode: 253, duration: 3.247s, episode steps: 158, steps per second:  49, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 2.013866, mae: 45.660446, mean_q: 92.069618\n",
      " 35959/50000: episode: 254, duration: 4.431s, episode steps: 215, steps per second:  49, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.292886, mae: 44.791569, mean_q: 90.420479\n",
      " 36148/50000: episode: 255, duration: 3.918s, episode steps: 189, steps per second:  48, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.068204, mae: 45.353622, mean_q: 91.400650\n",
      " 36307/50000: episode: 256, duration: 3.282s, episode steps: 159, steps per second:  48, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.695517, mae: 45.439861, mean_q: 91.609108\n",
      " 36498/50000: episode: 257, duration: 3.933s, episode steps: 191, steps per second:  49, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.939661, mae: 44.861042, mean_q: 90.297546\n",
      " 36659/50000: episode: 258, duration: 3.318s, episode steps: 161, steps per second:  49, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.791067, mae: 44.904217, mean_q: 90.552704\n",
      " 36822/50000: episode: 259, duration: 3.369s, episode steps: 163, steps per second:  48, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.811575, mae: 44.407162, mean_q: 89.405746\n",
      " 37046/50000: episode: 260, duration: 4.612s, episode steps: 224, steps per second:  49, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.928871, mae: 44.914558, mean_q: 90.330528\n",
      " 37271/50000: episode: 261, duration: 4.633s, episode steps: 225, steps per second:  49, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.634737, mae: 44.427452, mean_q: 89.380257\n",
      " 37425/50000: episode: 262, duration: 3.179s, episode steps: 154, steps per second:  48, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.265767, mae: 44.524044, mean_q: 89.649475\n",
      " 37626/50000: episode: 263, duration: 4.147s, episode steps: 201, steps per second:  48, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.019626, mae: 44.469597, mean_q: 89.621170\n",
      " 37805/50000: episode: 264, duration: 3.688s, episode steps: 179, steps per second:  49, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.208260, mae: 44.150505, mean_q: 88.890770\n",
      " 38017/50000: episode: 265, duration: 4.366s, episode steps: 212, steps per second:  49, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.089399, mae: 43.932842, mean_q: 88.492676\n",
      " 38182/50000: episode: 266, duration: 3.396s, episode steps: 165, steps per second:  49, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.139072, mae: 43.422161, mean_q: 87.487404\n",
      " 38385/50000: episode: 267, duration: 4.194s, episode steps: 203, steps per second:  48, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.108907, mae: 43.924141, mean_q: 88.489281\n",
      " 38648/50000: episode: 268, duration: 5.431s, episode steps: 263, steps per second:  48, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.898707, mae: 43.665176, mean_q: 88.018387\n",
      " 38829/50000: episode: 269, duration: 3.736s, episode steps: 181, steps per second:  48, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.648511, mae: 43.495037, mean_q: 87.704071\n",
      " 39037/50000: episode: 270, duration: 4.291s, episode steps: 208, steps per second:  48, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.181194, mae: 43.229233, mean_q: 87.143028\n",
      " 39203/50000: episode: 271, duration: 3.430s, episode steps: 166, steps per second:  48, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.048531, mae: 43.856205, mean_q: 88.214783\n",
      " 39430/50000: episode: 272, duration: 4.675s, episode steps: 227, steps per second:  49, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.998623, mae: 42.826130, mean_q: 86.357201\n",
      " 39647/50000: episode: 273, duration: 4.488s, episode steps: 217, steps per second:  48, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 1.975363, mae: 42.939606, mean_q: 86.568710\n",
      " 39834/50000: episode: 274, duration: 3.859s, episode steps: 187, steps per second:  48, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.893322, mae: 42.592384, mean_q: 86.000046\n",
      " 40076/50000: episode: 275, duration: 4.995s, episode steps: 242, steps per second:  48, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.954150, mae: 42.967823, mean_q: 86.600853\n",
      " 40324/50000: episode: 276, duration: 5.114s, episode steps: 248, steps per second:  48, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.062244, mae: 42.646011, mean_q: 85.959564\n",
      " 40480/50000: episode: 277, duration: 3.216s, episode steps: 156, steps per second:  49, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.150878, mae: 42.471230, mean_q: 85.550377\n",
      " 40658/50000: episode: 278, duration: 3.670s, episode steps: 178, steps per second:  49, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.054531, mae: 42.320789, mean_q: 85.325325\n",
      " 40928/50000: episode: 279, duration: 5.567s, episode steps: 270, steps per second:  48, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.042782, mae: 42.262619, mean_q: 85.086182\n",
      " 41124/50000: episode: 280, duration: 4.033s, episode steps: 196, steps per second:  49, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.784556, mae: 42.750656, mean_q: 86.202446\n",
      " 41406/50000: episode: 281, duration: 5.807s, episode steps: 282, steps per second:  49, episode reward: 282.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.418482, mae: 42.689728, mean_q: 86.132210\n",
      " 41574/50000: episode: 282, duration: 3.462s, episode steps: 168, steps per second:  49, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1.559204, mae: 42.275307, mean_q: 85.186852\n",
      " 41765/50000: episode: 283, duration: 3.932s, episode steps: 191, steps per second:  49, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.778202, mae: 42.309830, mean_q: 85.305542\n",
      " 41982/50000: episode: 284, duration: 4.465s, episode steps: 217, steps per second:  49, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.800491, mae: 42.317944, mean_q: 85.366974\n",
      " 42253/50000: episode: 285, duration: 5.583s, episode steps: 271, steps per second:  49, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 1.754506, mae: 42.170734, mean_q: 84.921600\n",
      " 42511/50000: episode: 286, duration: 5.315s, episode steps: 258, steps per second:  49, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.929896, mae: 42.330013, mean_q: 85.369308\n",
      " 42844/50000: episode: 287, duration: 6.853s, episode steps: 333, steps per second:  49, episode reward: 333.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.546436, mae: 41.848938, mean_q: 84.374779\n",
      " 43053/50000: episode: 288, duration: 4.293s, episode steps: 209, steps per second:  49, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.443795, mae: 42.183182, mean_q: 85.120224\n",
      " 43215/50000: episode: 289, duration: 3.334s, episode steps: 162, steps per second:  49, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.426507, mae: 42.197102, mean_q: 85.172455\n",
      " 43566/50000: episode: 290, duration: 7.216s, episode steps: 351, steps per second:  49, episode reward: 351.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.546606, mae: 42.307621, mean_q: 85.395943\n",
      " 43851/50000: episode: 291, duration: 5.864s, episode steps: 285, steps per second:  49, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.572277, mae: 42.111656, mean_q: 84.938164\n",
      " 44167/50000: episode: 292, duration: 6.499s, episode steps: 316, steps per second:  49, episode reward: 316.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.426207, mae: 41.966507, mean_q: 84.721497\n",
      " 44504/50000: episode: 293, duration: 6.951s, episode steps: 337, steps per second:  48, episode reward: 337.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.569907, mae: 42.008446, mean_q: 84.751633\n",
      " 44943/50000: episode: 294, duration: 9.029s, episode steps: 439, steps per second:  49, episode reward: 439.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.442223, mae: 42.212971, mean_q: 85.159332\n",
      " 45443/50000: episode: 295, duration: 10.259s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.420257, mae: 42.528103, mean_q: 85.807686\n",
      " 45634/50000: episode: 296, duration: 3.944s, episode steps: 191, steps per second:  48, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 1.410494, mae: 42.002319, mean_q: 84.606255\n",
      " 45966/50000: episode: 297, duration: 6.834s, episode steps: 332, steps per second:  49, episode reward: 332.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.567866, mae: 42.650967, mean_q: 85.988228\n",
      " 46466/50000: episode: 298, duration: 10.276s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.659782, mae: 42.737198, mean_q: 86.146202\n",
      " 46966/50000: episode: 299, duration: 10.273s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.985140, mae: 42.748096, mean_q: 86.134674\n",
      " 47466/50000: episode: 300, duration: 10.268s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.227970, mae: 43.032333, mean_q: 86.696861\n",
      " 47966/50000: episode: 301, duration: 10.268s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.789364, mae: 43.481609, mean_q: 87.691498\n",
      " 48209/50000: episode: 302, duration: 4.993s, episode steps: 243, steps per second:  49, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.405902, mae: 43.341209, mean_q: 87.357544\n",
      " 48709/50000: episode: 303, duration: 10.280s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.880629, mae: 43.710258, mean_q: 88.066528\n",
      " 49209/50000: episode: 304, duration: 10.263s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.216222, mae: 43.973610, mean_q: 88.534157\n",
      " 49709/50000: episode: 305, duration: 10.251s, episode steps: 500, steps per second:  49, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.945588, mae: 44.084560, mean_q: 88.756187\n",
      "done, took 1034.588 seconds\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 155.000, steps: 155\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 178.000, steps: 178\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 181.000, steps: 181\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n",
      "Average reward: 401.4\n"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# Test agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(f'Average reward: {np.mean(scores.history[\"episode_reward\"])}')\n",
    "\n",
    "# Save weights\n",
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6981249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 185.000, steps: 185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c4b01f6a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reload weights and test again\n",
    "dqn.load_weights('dqn_weights.h5f')\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
